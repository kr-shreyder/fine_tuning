{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "673d2cc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kris/Documents/sem7/MLOps/FineTuning/fine_tuning/venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Устройство: cpu, Сид зафиксирован: 42\n",
      "Классы: 3\n"
     ]
    }
   ],
   "source": [
    "# 1. Основные импорты PyTorch, timm и утилит\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import timm\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# 2. Установка пути для импорта вашего кода\n",
    "# (Это критически важно, чтобы Python мог найти fine_tuning_proj)\n",
    "# Предполагая, что вы запускаете ноутбук из папки 'notebooks'\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..'))) \n",
    "\n",
    "# 3. Импорт ваших пользовательских модулей\n",
    "# (Убедитесь, что 'fine_tuning_proj' соответствует имени вашей папки с исходниками)\n",
    "from fine_tuning_proj.config import GlobalConfig, DataConfig, ModelConfig, TrainConfig\n",
    "from fine_tuning_proj.utils import set_seed\n",
    "from fine_tuning_proj.dataset import get_transforms, get_dataloaders\n",
    "from fine_tuning_proj.plots import plot_learning_curves, plot_confusion_matrix\n",
    "\n",
    "# 4. Фиксация сидов и настройка устройства\n",
    "set_seed(GlobalConfig.SEED)\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# 5. Загрузка конфигурации из дата-классов\n",
    "data_cfg = DataConfig()\n",
    "model_cfg = ModelConfig()\n",
    "train_cfg = TrainConfig()\n",
    "\n",
    "print(f\"Устройство: {DEVICE}, Сид зафиксирован: {GlobalConfig.SEED}\")\n",
    "print(f\"Классы: {data_cfg.NUM_CLASSES}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b99708d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Имена классов, считанные из папок: ['Lily', 'Orchid', 'Peony']\n",
      "Размер обучающей выборки: 66\n",
      "Размер валидационной выборки: 12\n",
      "Размер тестовой выборки: 12\n"
     ]
    }
   ],
   "source": [
    "# 1. Получение трансформаций (train с аугментацией, val/test без)\n",
    "train_transforms, val_test_transforms = get_transforms(data_cfg)\n",
    "\n",
    "# 2. Создание DataLoader'ов\n",
    "train_loader, val_loader, test_loader = get_dataloaders(\n",
    "    data_cfg, \n",
    "    train_transforms, \n",
    "    val_test_transforms, \n",
    "    train_cfg.BATCH_SIZE\n",
    ")\n",
    "\n",
    "# 3. Верификация данных\n",
    "class_names = train_loader.dataset.classes\n",
    "print(f\"Имена классов, считанные из папок: {class_names}\")\n",
    "print(f\"Размер обучающей выборки: {len(train_loader.dataset)}\")\n",
    "print(f\"Размер валидационной выборки: {len(val_loader.dataset)}\")\n",
    "print(f\"Размер тестовой выборки: {len(test_loader.dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "815222f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    \"\"\"Выполняет один проход обучения.\"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    \n",
    "    for inputs, labels in dataloader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        correct_predictions += torch.sum(preds == labels.data)\n",
    "        \n",
    "    epoch_loss = running_loss / len(dataloader.dataset)\n",
    "    epoch_acc = correct_predictions.double() / len(dataloader.dataset)\n",
    "    return epoch_loss, epoch_acc.item()\n",
    "\n",
    "def evaluate_model(model, dataloader, device):\n",
    "    \"\"\"Оценивает модель на валидационной/тестовой выборке.\"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            \n",
    "            correct_predictions += torch.sum(preds == labels.data)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "    epoch_loss = running_loss / len(dataloader.dataset)\n",
    "    epoch_acc = correct_predictions.double() / len(dataloader.dataset)\n",
    "    return epoch_loss, epoch_acc.item(), np.array(all_labels), np.array(all_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a7ed4d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Стратегия Fine-Tuning для ResNet18 ---\n",
      "Этап 1: Заморожен Body, обучается только Head (fc layer).\n"
     ]
    }
   ],
   "source": [
    "# 1. Инициализация ResNet18\n",
    "model_name_cnn = model_cfg.MODEL_1_NAME\n",
    "model_cnn = timm.create_model(model_name_cnn, pretrained=True)\n",
    "\n",
    "# 2. Адаптация Head (замена последнего слоя на 3 класса)\n",
    "num_ftrs = model_cnn.fc.in_features\n",
    "model_cnn.fc = nn.Linear(num_ftrs, data_cfg.NUM_CLASSES)\n",
    "model_cnn = model_cnn.to(DEVICE)\n",
    "\n",
    "# 3. Стратегия: Сначала замораживаем все веса\n",
    "print(\"--- Стратегия Fine-Tuning для ResNet18 ---\")\n",
    "for param in model_cnn.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "# Размораживаем только классификационный Head ('fc' слой) для первого этапа обучения\n",
    "for param in model_cnn.fc.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "print(\"Этап 1: Заморожен Body, обучается только Head (fc layer).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "adc9c971",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa73bd74f3714e93892c36bff8e844bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training resnet18:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Эпоха 1: Train Loss=1.1140, Val Loss=1.1146, Val Acc=0.1667\n",
      "Эпоха 2: Train Loss=1.1074, Val Loss=1.1058, Val Acc=0.2500\n",
      "Эпоха 3: Train Loss=1.1090, Val Loss=1.1041, Val Acc=0.2500\n",
      "Эпоха 4: Train Loss=1.0704, Val Loss=1.0949, Val Acc=0.2500\n",
      "Эпоха 5: Train Loss=1.1030, Val Loss=1.0920, Val Acc=0.2500\n",
      "Эпоха 6: Train Loss=1.0980, Val Loss=1.0924, Val Acc=0.3333\n",
      "Эпоха 7: Train Loss=1.0925, Val Loss=1.0909, Val Acc=0.4167\n",
      "Эпоха 8: Train Loss=1.1138, Val Loss=1.0836, Val Acc=0.4167\n",
      "Эпоха 9: Train Loss=1.0976, Val Loss=1.0776, Val Acc=0.5000\n",
      "Эпоха 10: Train Loss=1.0979, Val Loss=1.0753, Val Acc=0.5000\n",
      "\n",
      "--- Размораживание Body: активируем последние слои ---\n",
      "Эпоха 11: Train Loss=1.0968, Val Loss=1.0771, Val Acc=0.5000\n",
      "Эпоха 12: Train Loss=1.0950, Val Loss=1.0753, Val Acc=0.5000\n",
      "Эпоха 13: Train Loss=1.0854, Val Loss=1.0877, Val Acc=0.4167\n",
      "Эпоха 14: Train Loss=1.0949, Val Loss=1.0850, Val Acc=0.5000\n",
      "Эпоха 15: Train Loss=1.1025, Val Loss=1.0730, Val Acc=0.5833\n"
     ]
    }
   ],
   "source": [
    "# Обучение ResNet18\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_cnn = torch.optim.AdamW(model_cnn.parameters(), lr=train_cfg.LEARNING_RATE)\n",
    "\n",
    "history_cnn = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': []}\n",
    "\n",
    "for epoch in tqdm(range(train_cfg.EPOCHS), desc=f\"Training {model_name_cnn}\"):\n",
    "    # 1. Обучение\n",
    "    train_loss, train_acc = train_one_epoch(model_cnn, train_loader, criterion, optimizer_cnn, DEVICE)\n",
    "    \n",
    "    # 2. Валидация\n",
    "    val_loss, val_acc, _, _ = evaluate_model(model_cnn, val_loader, DEVICE)\n",
    "    \n",
    "    # 3. Логирование (требование задания)\n",
    "    history_cnn['train_loss'].append(train_loss)\n",
    "    history_cnn['val_loss'].append(val_loss)\n",
    "    history_cnn['train_acc'].append(train_acc)\n",
    "    history_cnn['val_acc'].append(val_acc)\n",
    "    \n",
    "    print(f\"Эпоха {epoch+1}: Train Loss={train_loss:.4f}, Val Loss={val_loss:.4f}, Val Acc={val_acc:.4f}\")\n",
    "    \n",
    "    # --- ЭТАП 2: Размораживание (пример реализации подбора гиперпараметров) ---\n",
    "    # Например, разморозить последние 5 слоев после 10 эпох\n",
    "    if epoch == 9:\n",
    "        print(\"\\n--- Размораживание Body: активируем последние слои ---\")\n",
    "        # Размораживаем, например, последний блок и последний слой\n",
    "        for name, param in model_cnn.named_parameters():\n",
    "            if 'layer4' in name or 'fc' in name: \n",
    "                param.requires_grad = True\n",
    "        \n",
    "        # Обновляем оптимизатор, чтобы он увидел новые параметры\n",
    "        optimizer_cnn = torch.optim.AdamW(\n",
    "            filter(lambda p: p.requires_grad, model_cnn.parameters()), \n",
    "            lr=train_cfg.LEARNING_RATE / 10 # Снижаем LR для тонкой настройки\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae1ea8da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03470ec709ee4ec1b75f7b8e711e8622",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/346M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Модель: vit_base_patch16_224 загружена и адаптирована.\n",
      "--- Стратегия Fine-Tuning для ViT ---\n",
      "Этап 1: Заморожен Body, обучается только Head (head layer).\n"
     ]
    }
   ],
   "source": [
    "# --- Инициализация ViT (Vision Transformer) ---\n",
    "model_name_vit = model_cfg.MODEL_2_NAME # 'vit_base_patch16_224'\n",
    "model_vit = timm.create_model(model_name_vit, pretrained=True)\n",
    "\n",
    "# 1. Адаптация Head: У ViT классификационный слой обычно называется 'head.fc'\n",
    "num_ftrs_vit = model_vit.head.in_features\n",
    "model_vit.head = nn.Linear(num_ftrs_vit, data_cfg.NUM_CLASSES)\n",
    "model_vit = model_vit.to(DEVICE)\n",
    "print(f\"Модель: {model_name_vit} загружена и адаптирована.\")\n",
    "\n",
    "# 2. Стратегия: Замораживаем все, кроме Head\n",
    "print(\"--- Стратегия Fine-Tuning для ViT ---\")\n",
    "for param in model_vit.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "# Размораживаем только классификационный Head \n",
    "for param in model_vit.head.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "print(\"Этап 1: Заморожен Body, обучается только Head (head layer).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4530ac50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e3c7c4e375e40e1a4f1c8c3bf458779",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training vit_base_patch16_224:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Эпоха 1: Train Loss=1.3887, Val Loss=1.2035, Val Acc=0.3333\n",
      "Эпоха 2: Train Loss=1.3253, Val Loss=1.1242, Val Acc=0.4167\n",
      "Эпоха 3: Train Loss=1.3038, Val Loss=1.0640, Val Acc=0.4167\n",
      "Эпоха 4: Train Loss=1.1486, Val Loss=0.9987, Val Acc=0.4167\n",
      "Эпоха 5: Train Loss=1.2149, Val Loss=0.9395, Val Acc=0.5833\n",
      "Эпоха 6: Train Loss=1.1005, Val Loss=0.8896, Val Acc=0.5833\n",
      "Эпоха 7: Train Loss=1.1079, Val Loss=0.8458, Val Acc=0.5833\n",
      "Эпоха 8: Train Loss=1.1041, Val Loss=0.8067, Val Acc=0.5833\n",
      "Эпоха 9: Train Loss=0.9351, Val Loss=0.7716, Val Acc=0.6667\n",
      "Эпоха 10: Train Loss=0.9710, Val Loss=0.7337, Val Acc=0.6667\n",
      "\n",
      "--- Размораживание Body ViT: активируем последние 4 блока ---\n",
      "Эпоха 11: Train Loss=0.9177, Val Loss=0.4603, Val Acc=0.9167\n",
      "Эпоха 12: Train Loss=0.6545, Val Loss=0.3474, Val Acc=0.9167\n",
      "Эпоха 13: Train Loss=0.4897, Val Loss=0.2754, Val Acc=1.0000\n",
      "Эпоха 14: Train Loss=0.4063, Val Loss=0.2162, Val Acc=1.0000\n",
      "Эпоха 15: Train Loss=0.2529, Val Loss=0.1685, Val Acc=1.0000\n"
     ]
    }
   ],
   "source": [
    "# Обучение ViT\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_vit = torch.optim.AdamW(model_vit.parameters(), lr=train_cfg.LEARNING_RATE)\n",
    "\n",
    "history_vit = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': []}\n",
    "\n",
    "for epoch in tqdm(range(train_cfg.EPOCHS), desc=f\"Training {model_name_vit}\"):\n",
    "    # 1. Обучение\n",
    "    train_loss, train_acc = train_one_epoch(model_vit, train_loader, criterion, optimizer_vit, DEVICE)\n",
    "    \n",
    "    # 2. Валидация\n",
    "    val_loss, val_acc, _, _ = evaluate_model(model_vit, val_loader, DEVICE)\n",
    "    \n",
    "    # 3. Логирование\n",
    "    history_vit['train_loss'].append(train_loss)\n",
    "    history_vit['val_loss'].append(val_loss)\n",
    "    history_vit['train_acc'].append(train_acc)\n",
    "    history_vit['val_acc'].append(val_acc)\n",
    "    \n",
    "    print(f\"Эпоха {epoch+1}: Train Loss={train_loss:.4f}, Val Loss={val_loss:.4f}, Val Acc={val_acc:.4f}\")\n",
    "    \n",
    "    # --- ЭТАП 2: Размораживание ViT после 10 эпох ---\n",
    "    if epoch == 9:\n",
    "        print(\"\\n--- Размораживание Body ViT: активируем последние 4 блока ---\")\n",
    "        \n",
    "        # Для ViT размораживаем последние 4 блока трансформера (блоки 8, 9, 10, 11)\n",
    "        # ViT имеет 12 блоков, индексация с 0.\n",
    "        for name, param in model_vit.named_parameters():\n",
    "            if any(f'blocks.{i}' in name for i in range(8, 12)) or 'head' in name:\n",
    "                param.requires_grad = True\n",
    "        \n",
    "        # Обновляем оптимизатор, чтобы он увидел новые параметры и уменьшаем LR для тонкой настройки\n",
    "        optimizer_vit = torch.optim.AdamW(\n",
    "            filter(lambda p: p.requires_grad, model_vit.parameters()), \n",
    "            lr=train_cfg.LEARNING_RATE / 10 \n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6bfb98",
   "metadata": {},
   "source": [
    "\n",
    "### Выводы и Выбор Модели для ONNX\n",
    "\n",
    "Согласно заданию, необходимо сравнить две модели из разных семейств ($\\text{CNN}$ и $\\text{Transformer}$), обсудить компромиссы и выбрать лучшую для экспорта в $\\text{ONNX}$.\n",
    "\n",
    "**Сравнительная Таблица Метрик**\n",
    "\n",
    "| Модель | Семейство | Тестовая Точность (Accuracy) | Скорость Инференса (Ожидание) | Сложность |\n",
    "| :--- | :--- | :--- | :--- | :--- |\n",
    "| **ResNet18** | CNN (Сверточная сеть) | 0.5833 | Высокая (быстрее) | Ниже |\n",
    "| **ViT (Transformer)** | Transformer | 1.0000 | Умеренная (медленнее) | Выше |\n",
    "\n",
    "**Обоснование Выбора**\n",
    "\n",
    "На основе полученных результатов:\n",
    "\n",
    "**Если $\\text{ViT}$ показал лучшую точность:**\n",
    "Лучшей моделью выбрана **ViT ($\\text{vit\\_base\\_patch16\\_224}$)**.\n",
    "* **Причина:** ViT превосходит $\\text{ResNet18}$ в итоговой точности на тестовой выборке, что указывает на лучшую способность захватывать глобальные признаки изображения.\n",
    "* **Компромисс:** Мы принимаем немного более высокую вычислительную сложность и, возможно, большее время инференса на $\\text{CPU}$ ради более высокой производительности классификации.\n",
    "\n",
    "Модель $\\text{ViT}$ будет экспортирована в $\\text{ONNX}$ и использована в локальном приложении."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
